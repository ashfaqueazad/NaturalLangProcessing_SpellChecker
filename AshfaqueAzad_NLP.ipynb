{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (10 Marks)\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#Importing the text file\n",
    "filename = 'C:/####/holbrook.txt'\n",
    "\n",
    "#Reading the file\n",
    "textFile=open(filename,\"r\")\n",
    "data = textFile.readlines()\n",
    "\n",
    "#Counting the total number of lines\n",
    "num_lines = sum(1 for line in open(filename))\n",
    "\n",
    "#l gives us the total lines\n",
    "l=num_lines-1\n",
    "\n",
    "#Creating an empty list \n",
    "sentenceList=[]\n",
    "\n",
    "#Filling the list with data\n",
    "for i in range(0,l):\n",
    "    sentenceList.append(nltk.word_tokenize(data[i]))\n",
    "\n",
    "#Creating an empty list which would contain the original sentences\n",
    "ORG=[]\n",
    "\n",
    "#Creating an empty list which would contain the corrected sentences\n",
    "COR=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "from nltk import word_tokenize\n",
    "import more_itertools as mit\n",
    "import itertools\n",
    "from itertools import chain\n",
    "\n",
    "#Creating an empty list which would contain the location of misspelled words\n",
    "INDEX=[]\n",
    "\n",
    "#splitFunction breaks the data into correctly and incorrectly spelled sentences\n",
    "def splitFunction(tokens):\n",
    "    #checking the no. of tokens\n",
    "    tokens_list = sum(1 for line in tokens)\n",
    "    slashappender=[]\n",
    "    original=[]\n",
    "    corrected=[]\n",
    "    index=[]\n",
    "    #splitting tokens on the basis of |\n",
    "    for i in range(0,tokens_list-1):\n",
    "        slashappender.append(tokens[i].split(\"|\"))\n",
    "    for i in range(0,tokens_list-1):\n",
    "        #if the token does not contain a slash\n",
    "        #add the word to the original and corrected lists\n",
    "        if(sum(1 for line in slashappender[i])==1):\n",
    "            string1=slashappender[i]\n",
    "            original.append(string1)\n",
    "            corrected.append(string1)\n",
    "        else:\n",
    "            sublistlength=sum(1 for line in slashappender[i])\n",
    "            for j in range(0,sublistlength-1):\n",
    "                #word before slash\n",
    "                string1=slashappender[i][j]\n",
    "                #word after slash\n",
    "                string2=slashappender[i][j+1]\n",
    "                #add the tokens as list to the respective lists\n",
    "                original.append(string1.split())\n",
    "                corrected.append(string2.split())\n",
    "                #add the index of the misspelled word/s\n",
    "                index.append(i)\n",
    "    ORG.append(original)\n",
    "    COR.append(corrected)\n",
    "    INDEX.append(index)\n",
    "\n",
    "#splitFunction(tokens)\n",
    "#splitFunction(tokens2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loop passes each of the sentences in the list to the splitFunction\n",
    "for i in range(0,l):\n",
    "    splitFunction(sentenceList[i])\n",
    "\n",
    "    \n",
    "#This nested loop turns each word in the ORG and COR into lower case\n",
    "for i in range(0,l):\n",
    "    for j in range(0,len(ORG[i])):\n",
    "        if(len(COR[i][j])!=0 and len(ORG[i][j])!=0):\n",
    "            ORG[i][j][0]=ORG[i][j][0].lower()\n",
    "            COR[i][j][0]=COR[i][j][0].lower()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#Making a dictionary\n",
    "dictionary = []\n",
    "dictionary = defaultdict(list)\n",
    "#This contains information of ORG, COR, and INDEX in each of its index\n",
    "for i in range(0,len(ORG)):\n",
    "    dictionary[i].append(ORG[i])\n",
    "    dictionary[i].append(COR[i])\n",
    "    dictionary[i].append(INDEX[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and test datasets\n",
    "testingData = []\n",
    "testingData = defaultdict(list)\n",
    "#Contains first 100 dictionary items\n",
    "for i in range(0,len(dictionary)-1116):\n",
    "    testingData[i]=dictionary[i]\n",
    "    \n",
    "\n",
    "trainingData = []\n",
    "trainingData = defaultdict(list)\n",
    "k = 0\n",
    "#Contains all the dictionary items save the first hundred\n",
    "for i in range(99,len(dictionary)):\n",
    "    trainingData[k]=dictionary[i]\n",
    "    k = k+1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2** (10 Marks): \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and bigrams (sequences of two words) from the corrected *training* sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "def unigram(word):\n",
    "    # Write your code here.\n",
    "    #This function takes in a word \n",
    "    #and checks the whole corrected training corpus for its presence.\n",
    "    count=0\n",
    "    for i in range(0,len(trainingData)):\n",
    "        for j in range(0,len(trainingData[i][1])):\n",
    "            #Checks if the corpus contains a word\n",
    "            #And if so, whether it matches the word\n",
    "            #And if so, increments the count by 1\n",
    "            if(len(trainingData[i][1][j])!=0 and word==trainingData[i][1][j][0]):\n",
    "                count=count+1\n",
    "    return count  \n",
    "    \n",
    "\n",
    "def bigram(words):\n",
    "    # Write your code here.\n",
    "    count=0\n",
    "    for i in range(0,len(trainingData)):\n",
    "        for j in range(0,len(trainingData[i][1])):\n",
    "            #This checks if the corpus not just contains a word\n",
    "            #But also a word following the word\n",
    "            #If a match occurs, count is incremented by 1.\n",
    "            if((j+1)<len(trainingData[i][1]) and len(trainingData[i][1][j])!=0 and len(trainingData[i][1][j+1])!=0 and words==(trainingData[i][1][j][0]+\" \"+trainingData[i][1][j+1][0]) ):\n",
    "                count=count+1\n",
    "    return count\n",
    "\n",
    "# Test your code with the following\n",
    "\n",
    "print(unigram('me'))\n",
    "#assert(bigram(\"my mother\")==17)\n",
    "print(bigram(\"my mother\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code with the following\n",
    "assert(unigram('me')==83)\n",
    "assert(bigram('my mother')==17)\n",
    "assert(bigram('you did')==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 3** (15 Marks): \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "#An empty list\n",
    "bagofWords=[]\n",
    "#This nested loop appends to the bagofWords\n",
    "#All the words that are present in the COR \n",
    "for i in range(0,len(COR)-1):\n",
    "    for j in range(0,len(COR[i])):\n",
    "        bagofWords.append(COR[i][j])\n",
    "\n",
    "bagofWords=list(chain.from_iterable(bagofWords))\n",
    "\n",
    "#From this, we get the unique correct words present in the corpus\n",
    "uniqueWords=set(bagofWords)\n",
    "\n",
    "#Turning it into a list\n",
    "uniqueWords=list(uniqueWords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mind', 'mine']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "def get_candidates(token):\n",
    "    # Write your code here.\n",
    "    #List containing edit distances\n",
    "    editDistanceList=[]\n",
    "    #list containg possible candidates for right spelling\n",
    "    candidates=[]\n",
    "    for i in range(0,len(uniqueWords)):\n",
    "        editDistanceList.append(edit_distance(token,uniqueWords[i]))\n",
    "    #Getting the minimum value for edit distance\n",
    "    minimalValue=min(editDistanceList)\n",
    "    for i in range(0,len(uniqueWords)):\n",
    "        if(edit_distance(token,uniqueWords[i])==minimalValue):\n",
    "            #appending the word/s with the minimum edit distances\n",
    "            candidates.append(uniqueWords[i])\n",
    "    return candidates\n",
    "        \n",
    "# Test your code as follows\n",
    "get_candidates(\"minde\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_candidates(\"minde\") == ['mind', 'mine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (15 Marks):\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *bigram probability*. That is the candidate should be selected using the previous and following word in a bigram language model. Thus, if the $i$th word in a sentence is misspelled we should use the following to rank candidates:\n",
    "\n",
    "$$p(w_{i+1}|w_i) p(w_i|w_{i-1})$$\n",
    "\n",
    "For the first and last word of the sentence use only the conditional probabilities that exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "# make a dictionary of all correct words from holbrook\n",
    "dictionary1 = []\n",
    "for i in range(len(COR)):\n",
    "    dictionary1.append([x for x in COR[i]])\n",
    "\n",
    "dictionary1 = list(chain.from_iterable(dictionary1))\n",
    "\n",
    "#This function gives the correct sentence based on conditional probability\n",
    "def correct(sentence):\n",
    "    correctSentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        #if sentence in dictionary1, then append\n",
    "        if(sentence[i] in dictionary1):\n",
    "            correctSentence.append(sentence[i])\n",
    "        else:\n",
    "            #get candidates for that token\n",
    "            get_cand = list((get_candidates(sentence[i])))\n",
    "            if(i < (len(sentence)-1) and i > 0):\n",
    "                #get the words before and after the candidate word\n",
    "                prefix = sentence[i-1]\n",
    "                suffix = sentence[i+1]\n",
    "                #the following list will contain conditional probabilities\n",
    "                conditional_probability = []\n",
    "                for a in range(0,len(get_cand)):\n",
    "                    #Appends the conditional probability\n",
    "                    conditional_probability.append(((bigram(prefix + \" \" + get_cand[a]))/(dictionary1.count(prefix)+0.1))*((bigram(get_cand[a] + \" \" + suffix))/(dictionary1.count(get_cand[a])+0.1)))\n",
    "                #Gets the index of the maximum probability value\n",
    "                highest_prob_ind = conditional_probability.index(max(conditional_probability))\n",
    "                #get the candidate with the corresponding maximum highest probability \n",
    "                #and append to the correct Sentence\n",
    "                correctSentence.append(get_cand[highest_prob_ind])\n",
    "            else:\n",
    "                #if the first word in the sentence is the wrong word \n",
    "                index = sentence.index(sentence[i])\n",
    "                #checks if the index is 0 and the sentence consists of more than 1 word\n",
    "                if(index == 0 and len(sentence)>1):\n",
    "                    suffix = sentence[i+1]\n",
    "                    conditional_probability = []\n",
    "                    for a in range(0,len(get_cand)):\n",
    "                        #calculates the probability\n",
    "                        conditional_probability.append((bigram(get_cand[a] + \" \" + suffix))/(dictionary1.count(get_cand[a])+0.1))\n",
    "                    \n",
    "                    #appends the word with maximum probability\n",
    "                    correctSentence.append(get_cand[conditional_probability.index(max(conditional_probability))])\n",
    "                #this checks for the last word in the sentence being the wrong word\n",
    "                elif(len(sentence)>1):\n",
    "                    prefix = sentence[i-1]\n",
    "                    cdf = []\n",
    "                    for a in range(0,len(get_cand)):\n",
    "                        cdf.append((bigram(prefix + \" \" + get_cand[a]))/(dictionary1.count(prefix)+0.1 ))\n",
    "                    #appends the word with maximum probability\n",
    "                    correctSentence.append(get_cand[cdf.index(max(cdf))])\n",
    "                else:\n",
    "                    #If the sentence consists of only 1 word\n",
    "                    #This appends the candidate occuring first in the list\n",
    "                    x=list((get_candidates(sentence[i])))[0]\n",
    "                    correctSentence.append(x)\n",
    "      #returns the correct sentence      \n",
    "    return correctSentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "#Note: arguments passed must be in lower case\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 5** (10 Marks): \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9423076923076923\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    count=0\n",
    "    totalCount=0\n",
    "    for i in range(len(dictionary)):\n",
    "        for j in range(len(dictionary[i][2])):\n",
    "            if(len(dictionary[i][2])> 0 and i<99):\n",
    "                m=correct(list(chain.from_iterable(test[i][0])))\n",
    "                if(m[j]==dictionary[i][1][j][0]):\n",
    "                    count=count+1\n",
    "                    totalCount=totalCount+1\n",
    "                else:\n",
    "                    totalCount=totalCount+1\n",
    "    return(count/totalCount)\n",
    "\n",
    "print(accuracy(testingData))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **Task 6 (35 Marks):**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3\n",
    "\n",
    "_Marks will be awarded based on how interesting the proposed improvement is. Please provide a short text describing what you intend to do and why. Full marks for this section may be obtained without an implementation, but an implementation is preferred._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The modified algorithm is thus:\n",
    " 1. Get the number of all possible bigrams in the training dataset. Let us name it countOfAllPossibleBigrams\n",
    " 2. repeat for every word in the sentence:\n",
    "     \n",
    "     if(the incorrect word is neither the first or the last word and is not the only word in the sentence):\n",
    "         \n",
    "       a. Get the bigram frequency of the candidate (replacement for the incorrect word) with the word preceding it, add 1 to it, and divide it by countOfAllPossibleBigrams.\n",
    "       \n",
    "       b. Get the bigram frequency of the candidate with the word following it, add 1 to it, and divide it by countOfAllPossibleBigrams.\n",
    "       \n",
    "       c. Multiply the contents of 2a. and 2b.\n",
    "       \n",
    "       d. if( the probability == 0):\n",
    "            Take care of zero probability by giving the probability as 0.1/countOfAllPossibleBigrams. This is done so as to penalize bigrams which do not occur in the corpus.\n",
    "     \n",
    "     if(the incorrect word is the first word):\n",
    "       \n",
    "       e. calculate the bigram frequency of the candidate with the word following it, add 1 to it , and divide it by countOfAllPossibleBigrams.\n",
    "       \n",
    "       f. GOTO 2d.\n",
    "     \n",
    "     if(the incorrect word is the last word):\n",
    "       \n",
    "       g. calculate the bigram frequency of the candidate with the word preceding it, add 1 to it , and divide it by countOfAllPossibleBigrams.\n",
    "       \n",
    "       h. GOTO 2d.\n",
    "       \n",
    "     if(there is only one word in the sentence):\n",
    "       \n",
    "       i. return the first candidate in the list.\n",
    " 3. Return the corrected sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 7 (5 Marks):**\n",
    "\n",
    "Repeat the evaluation of your new algorithm and show that it outperforms the algorithm from Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19481"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we find the possible number of bigrams in the trainingData\n",
    "counting=[]\n",
    "for i in range(len(trainingData)):\n",
    "    length=len(trainingData[i][1])\n",
    "    counting.append(length-1)\n",
    "sum(counting)\n",
    "#counting gives us the sum of all possible consecutive bigrams\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'dad', 'works', 'at', 'melton']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "# make a dictionary of all correct words from holbrook\n",
    "dictionary1 = []\n",
    "for i in range(len(COR)):\n",
    "    dictionary1.append([x for x in COR[i]])\n",
    "\n",
    "dictionary1 = list(chain.from_iterable(dictionary1))\n",
    "\n",
    "def correct_v2(sentence):\n",
    "    correctSentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        if(sentence[i] in list(chain.from_iterable(dictionary1))):\n",
    "            correctSentence.append(sentence[i])\n",
    "        else:\n",
    "            get_cand = list((get_candidates(sentence[i])))    \n",
    "            if(i < (len(sentence)-1) and i > 0):\n",
    "                \n",
    "                prefix = sentence[i-1]\n",
    "                suffix = sentence[i+1]\n",
    "                conditional_probability = []\n",
    "                for a in range(0,len(get_cand)): \n",
    "                    #Here we calculate probabilities as follows\n",
    "                    #Get the bigram frequency of the candition and the word preceding it and divide it by the total\n",
    "                    #possible bigrams in the correct section of the traning data\n",
    "                    #Multiply it by the bigram frequency of the word followed by the candidate and\n",
    "                    #divide it by the sum(counting) i.e. the number of all possible bigrams  \n",
    "                    x=((bigram(prefix + \" \" + get_cand[a]) +1)/sum(counting))*((bigram(get_cand[a] + \" \" + suffix)+1)/sum(counting))\n",
    "                    if(x>0):\n",
    "                        conditional_probability.append(((bigram(prefix + \" \" + get_cand[a]) +1)/sum(counting))*((bigram(get_cand[a] + \" \" + suffix)+1)/sum(counting)))\n",
    "                    else:\n",
    "                        #This appends 1/sum(counting) in case of zero probability\n",
    "                        conditional_probability.append(0.01/sum(counting))\n",
    "                highest_prob_ind = conditional_probability.index(max(conditional_probability))\n",
    "                correctSentence.append(get_cand[highest_prob_ind])\n",
    "            else:\n",
    "                index = sentence.index(sentence[i])\n",
    "                if(index == 0 and len(sentence)>1):\n",
    "                    suffix = sentence[i+1]\n",
    "                    conditional_probability = []\n",
    "                    for a in range(0,len(get_cand)):\n",
    "                        #if it is the first word which is incorrect in a sentence\n",
    "                        #get its bigram frequency with the word following it and divide it by all possible ones.\n",
    "                        y=(bigram(get_cand[a] + \" \" + suffix))\n",
    "                        x=y+1/sum(counting)\n",
    "                        if(x>0):\n",
    "                            conditional_probability.append((bigram(get_cand[a] + \" \" + suffix)+1)/sum(counting))\n",
    "                        else:\n",
    "                            conditional_probability.append(0.01/sum(counting))\n",
    "                    correctSentence.append(get_cand[conditional_probability.index(max(conditional_probability))])\n",
    "                elif(len(sentence)>1):\n",
    "                    prefix = sentence[i-1]\n",
    "                    cdf = []\n",
    "                    for a in range(0,len(get_cand)):\n",
    "                        #get bigram with the word preceding it \n",
    "                        #and divide by the number of all possible occurences of bigrams\n",
    "                        y=bigram(prefix + \" \" + get_cand[a])\n",
    "                        x=y+1/sum(counting)\n",
    "                        if(x>0):\n",
    "                            cdf.append((bigram(prefix + \" \" + get_cand[a])+1)/sum(counting))\n",
    "                        else:\n",
    "                            cdf.append(0.01/sum(counting))\n",
    "                    correctSentence.append(get_cand[cdf.index(max(cdf))])\n",
    "                else:\n",
    "                    #if the sentence just consists of a word (which itself is incorrect)\n",
    "                    #select the first candidate as the changed word\n",
    "                    x=list((get_candidates(sentence[i])))[0]\n",
    "                    correctSentence.append(x)\n",
    "            \n",
    "    return correctSentence\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "correct_v2(['my', 'dad', 'works', 'at', 'meltn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9519230769230769\n"
     ]
    }
   ],
   "source": [
    "#Checking the accuracy of the algorithm modified\n",
    "def accuracy_v2(test):\n",
    "    count=0\n",
    "    totalCount=0\n",
    "    for i in range(len(dictionary)):\n",
    "        for j in range(len(dictionary[i][2])):\n",
    "            if(len(dictionary[i][2])> 0 and i<99):\n",
    "                m=correct_v2(list(chain.from_iterable(test[i][0])))\n",
    "                if(m[j]==dictionary[i][1][j][0]):\n",
    "                    count=count+1\n",
    "                    totalCount=totalCount+1\n",
    "                else:\n",
    "                    totalCount=totalCount+1\n",
    "    return(count/totalCount)\n",
    "\n",
    "print(accuracy_v2(testingData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy by the new method has increased by about 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASHFAQUE AZAD "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
